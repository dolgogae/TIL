스토리지

/dev: 물리적 장치에 연결하기 위한 인터페이스
/proc + /sys: 현재는 proc및 sys가 분리
    - proc:  프로세스 상태정보를 가지고 있음(RO)
    - sys: 커널 모듈과 관련되어있는 디렉터리()
        * lsmod, modprobe, insmod
  |
  |
  interrupts: 문제가 생기면 확인

FS H/P: CFS(x), RHEL 7(8이 최적)(미만은 비권장)

(/proc/mounts == mount) 명령어 같음

/sys/block/vd~/queue
- 예상했던 속도나 퍼포먼스가 안나올때 여기를 본다.
- BFQ:(BFS: Brain Fuck Scheduler의 영향을 받음)
- mq-deadline: 최대한 결과를 빠르게 도출하는게 목적

mq 

0~3: 4cores

Linux Kernel 2.6, 3.2 미만으로는, I/O는 0,1번만 사용함
리눅스는 멀티프로세싱이 주목적
유닉스는 싱글프로세싱이 주목적

/dev/vd*: 가상머신
    - virtio-scsi: vda-> sda으로 변환 하지만, 반가상화 기능 지원
      * vmware

/dev/sd*: SAS, SSD, M.2

M.2: /dev/nvme
/dev/dm, dm-mpath: SAN스토리지 연결시 많이 보이는 장치.
                    멀티패스가 구성되면 mpath라는 이름으로 많이 보임

mount -t vfat -t ext3 ---> mount /dev/vda2 /mnt/test

VFS: 알아서 F/S타입 확인 후 연결, 하지만 커널 영역에서 동작하기 때문에 느림.
Fuse: 유저 영역에서 다시 드라이버 재구성하여 더 높은 속도 및 메모리 기능 제공.
    - ntfs, hpfs, ufs, xfs, btrfs
    - CPU하고 메모리를 많이 사용함.

UUID: Superblock에 저장이 되어 있음. 모든 파일 시스템은 S/B가 엄청 중요함

하드웨어 기반의 레이드 혹은 컨트롤러 장치를 사용하는 경우, 
과연 LVM2같은 기능으로 파티션을 다시 재구성이 필요한가? **확장성

하드웨어 레이드 + LVM2: 비권장, 이유는 확장이 안됨.

LVM: CPU를 많이 사용함. lvm(10개, 속도, 메모리 및 업체, 생산날짜)
                     하드웨어 기반으로 어레이 구성해서 사용하는 경우 크게 영향 없음.

                     CPU를 덜 사용하기 위해서 하드웨어 어레이 사용. LVM2 CPU+MEM사용

LVM2(IBM AIX)
---------------
* 확장가능
* 백업
- 성능이 떨어짐
  * meta를 참조함. 크기 변경 및 영역 정보 수정시
  * 추상 개념

pvcreate: 물리적 장치에 signaturing

vgcreate(pvcreate)

lvcreate: devicemapper로 장치가 생성

       디스크 혹은 파티션 일부분 --> 권장
                    kpartx, partprobe
                             전체 디스크 이미지 갱신 --> 커널에 디스크 정보 전달
OS ---> running ---> disk 추가
                    기존 디스크 사용중

gdisk: for EFI디스크 파티션
        - 0~4096,8192 보통 맨 뒷쪽에 백업 파티션 하나 생성
        - 리눅스 서맃시 백업 파티션 구성 안해줌
        - 일반적으로 128개 파티션 생성 가능

fdisk: for MBR 디스크 파티션, 구형이고 쉽다. 대신 파티션 갯수가 제한이 있음
        - 0~2048,4096영역에 파티션 정보 저장(2048가지는 부트로더가 저장되는 장소)
        - 일반적으로 34~38개 정도 생성 가능

