curl http://localhost:8001/api/v1/pod`

API를 좀 더 쉽게 사용하기 위해 kubectl을 사용
kubectl 클라이언트 버전이 쿠버네티스 API 버전과 맞아야 한다.


kubectl apply, get, describe, delete

kubectl apply: 명령 요청/적용

- kubectl apply -f /path/to/my.yaml : 명령 요청 / 적용
- kubectl get pod <pod_name>
- kubectl describe pod <pod_name> : (자세한 정보)이벤트 오류 메세지도 뜸
- kubectl delete pod <pod_name> : 오브젝트 삭제

- kubectl get nodes -o wide : 클러스터 모든 노드 나열
- kubectl get pods --all-namespaces

- kubectl config get-contexts
- kubectl config use-context <namespace>

- kubectl get events : 쿠버네티스 문제 해결

- kubectl apply -f [<file> | <directory> | <url>]
  : 파일에서 새 리소스 생성 및 기존 리소스 업데이트

- kubectl create : 파일에서 새로운 리소스 생성
- kubectl replace : 파일에서 기존의 리소스를 업데이트
- kubectl edit : 기본 에디터를 상요해 기존의 리소스 업데이트
- kubectl patch

- kubectl log -c <container_name>
  : -c: 포드에 컨테이너가 2개 이상일때

쿠버네티스 포드 단계
- Pending : 리소스가 부족할때
- Running
- Succeeded : 성공적으로 종료
- failed : 오류
- unknown : 시스템 문제

====================================================

lab3

# echo "source <(kubectl completion bash)" >> ~/.bash_profile

# kubectl cluster-info --help
# kubectl cluster-info
# kubectl get nodes -o wide
# kubectl describe nodes sa-master-01
# kubectl get pods
# kubectl get pods --all-namespaces
# kubectl version --short

====================================================

기본 쿠버네티스 오브젝트
- Pods
- Replicasets
- Deployments
- Services: 네트워크 형태
- Namesapces: 네트워크 policy, 리소스 할당의 단위 
- Labels: 쿠버네티스 안에서 생성되고 관리된 오브젝트를 손쉽게 선택하기 위해서 붙혀놓은 논리적인 레이블

Pod 생성 목적
컨테이너를 추상화 하기 위한 목적.
한가지 용도로 pod를 사용하는 것이 좋다.

사이드카 컨테이너
- 일반적으로 로깅이나 모니터링과 같은 다른 시스템에 정보를 전달하는 데 사용되는 로깅 또는 메트릭 포드를 지칭하는데 사용된다.

포드 - 워커
포드는 쿠버네티스 워커 노드 내부에서 실행되며 하나 이상의 컨테이너 프로세스를 실행할 수 있습니다.

** Replicasets
동일한 포드를 여러개 띄우는 것 -> 스케줄러 따라 같은 워커노드나 다른 워커노드에 띄워짐


ex) 20개의 NGINX 서버를 띄워야할 경우
: 레플리카 셋은 업데이트가 어렵다
--> Deployments

** Deployments
Replicasets을 단위로 배포하는 용도
Replicasets에 strategy가 추가됨 - RollingUpdate
다운타임 없이 새 버전의 이미지를 배포.

Services
일반적으로 Deployments가 끝나면 네트워크가 분리되어 접근 불가능
Services로 네트워크를 부여
--예시
서비스 유형: LoadBalancer
포트 80에서 수신 대기
app 라벨 nginx를 가진 포드에서 트래픽 균형이 맞춘다
프래픽이 포드의 대상 포트 80으로 전송된다.

apiVersion: v1
kind: Service
metadata:
  name: my-nginx-service
spec:
  type: LoadBalancer
  selector:
    app: nginx
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80


cluster ip는 내부 통신으로만 이용

** namespace
클러스터 리소스를 분할하는데 사용되는 쿠버네티스 구조이다
다중 사용자 액세스 지원
리소스는 리소스 할당량을 사용하여 제어된다.

** label
레이블은 사용자 정의 특성을 전달하는 포드에 부착된 키-값 쌍이다. 클러스터 전체의 키-값 쌍을 표준화
레이블 선택기(selector)를 사용하여 특정 레이블이 있는 포드를 선택하고 서비스 또는 복제 컨트롤러를 적용할 수 있습니다(기능적이거나 조직적일 수 있음).

주의) 중의적으로 사용하지 말고 하나의 레이블이 한가지 의미만 가지게 app: blog-frontend -> app:blog  tier:frontend

워크 플로우 -> p123

Deployments에서 소유하는 Replicasets는 수동으로 관리하지 않는다.

RollingUpdate 배포
default 값
새 Replicaset가 작성된 후 이전 Replicaset의 크기를 줄여서 크기 조정
1.13.1 -> 1.13.2
1. 1.13.2가 새버전으로 Pod가 배포된다.
2. 기존의 파드(1.13.1) 삭제
3. 새버전(1.13.2)을 배포
4. 위와 같이 반복해서 무도 교체 될때까지 업데이트
-> 일시적으로 파드가 증가해서 리소스가 좀 더 많이 쓰이는 방법
-> 끊기지 않고 업데이트 가능

Recreate 배포
먼저 기존 Replicaset의 기존 포드를 모두 제거
새 Replicaset에 배포 진행
1. 기존을 완전히 죽임 : 이 순간에 endpoint 접근 불가로 서비스 안됨
2. 새버전 배포

**RollingUpdate / Recreate 활용
Canary 배포
1. 새버전을 하나 배포한 뒤, 일시적으로 같이 서비스 해본다.: 충분히 테스트가 가능
2. 이상현상 발견되면 Canary 삭제 하면 된다.
3. 정상적이면 그대로 배포 진행

Blue/Green 배포
1. 완전 두가지 버전을 배포한다.(구버전 1세트/ 신버전 1세트)
2. 두가지 모두 LB를 통해서 나눈다.
3. 정상적이면 구버전 삭제

kubectl rollout [pause | resume] deployment <name>

# kubectl apply -f deployment.yaml

# kubectl rollout undo deployment <name>

=========================================================
lab 5

cat -n new-webapp.sh

file: gowebapp-new-deployment.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gowebapp
  labels:
    app: gowebapp
    tier: frontend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: gowebapp
      tier: frontend
  template:
    metadata:
      labels:
        app: gowebapp
        tier: frontend
    spec:
      containers:
      - name: gowebapp
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: mypassword
        image: sa-master-01.vclass.local:443/gowebapp:v1.1
        ports:
        - containerPort: 80

```    
// 실행후 잠깐 멈춤
# kubectl apply -f gowebapp-new-deployment.yaml && sleep 2 && kubectl rollout pause deployment gowebapp

# kubectl get pods
# kubectl get replicasets
# kubectl describe deployment gowebapp

// 다시 실행 시작
# kubectl describe deployment gowebapp


** canary

canary-deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: canary
  labels:
    app: gowebapp
    tier: frontend
    track: canary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gowebapp
      tier: frontend
      track: canary
  template:
    metadata:
      labels:
        app: gowebapp
        tier: frontend
        track: canary
    spec:
      containers:
      - name: gowebapp
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: mypassword
        image: sa-master-01.vclass.local:443/gowebapp:v1.1
        ports:
        - containerPort: 80



# kubectl apply -f canary-deployment.yaml

# kubectl get pods -o wide
# curl localhost:port

# cat -n canary-service.yaml

apiVersion: v1
kind: Service
metadata:
  name: canary
  labels:
    app: gowebapp
    tier: frontend
spec:
  typd: NodePort
  ports:
  -ports: 80
  selector:
    app: gowebapp
    tier: frontend
    track: canary


# kubectl apply -f canary-service.yaml

==================================================

Probe
프로브는 컨테이너에서 kubelet에 의해 주기적으로 수행되는 진단입니다. 컨테이너에 대한 활성 및 준비 상태 프로브를 구성할 수 있습니다.

Probe Handler
진단을 수행하기 위해 kubelet은 컨테이너에 의해 구현된 핸들러를 호출합니다. 다음 유형의 핸들러를 사용할 수 있습니다.
• Exec: 
  – 컨테이너 내부에서 명령을 실행합니다. 
  – 성공: 명령의 리턴 코드가 0입니다.
• TCPSocket 
  – TCP 확인. 
  – 성공: 포트가 열려 있고 연결을 수락합니다.
• HTTPGet 
  – URL에 대해 HTTP GET을 호출합니다. 
  – 성공: 모든 2xx 또는 3xx HTTP 응답


- 캐시 초기화
- JVM이 성공적으로 시작되었는지 확인
  : URL에 대한 라이브니스 프로브를 지정하거나, 로그에서 특정 줄을 찾습니다.
- 컨테이너 유지관리

리소스 관리: CPU 값 정의

오버 커밋 될때, Pending 시킨다.

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gowebapp
  labels:
    app: gowebapp
    tier: frontend
spec:
  replicas: 2
  selector:
    matchLabels:
      app: gowebapp
      tier: frontend
  template:
    metadata:
      labels:
        app: gowebapp
        tier: frontend
    spec:
      containers:
      - name: gowebapp
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: mypassword
        image: sa-master-01.vclass.local:443/gowebapp:v1.1
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
```

// 오버 커밋
# kubectl scale --replicas=10 deployment gowebapp
# kubectl scale --replicas=20 deployment gowebapp

몇개는 올라오지 않음

file: exec-liveness.yaml
```
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: exec-liveness
spec:
  containers:
  - name: liveness
    image: sa-master-01.vclass.local:443/busybox:1
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep infinity
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSecond: 5
```
# kubectl apply -f exec-liveness.yaml

# kubectl describe pod exec-liveness

# kubectl exec exec-liveness -- rm /tmp/healthy

# kubectl describe pod exec-liveness


==========================================

쿠버네티스 네트워킹
 
플러그 인
• Kubenet 
• Flannel/Calico   : 젤 많이 쓰임
• Weave Net 
• Common VMware Tanzu choices:
    Contour
    NSX-T Data Center 
    NSX Advanced Load Balancer (Avi) 
    Antrea 
    Project Calico 
    HAProxy

오브젝트 네임을 가지고 통신을 한다.(ip는 새로 띄워지면서 바뀔수도 있다.)
프라이빗 네트워크를 사용한다.

Service는 쿠버네티스의 리소스 입니다.
- 포드 그룹을 위해 L4 로드 밸런싱을 수행한다.
- 클러스터 내부 DNS를 사용해서 서비스를 검색한다.

여러 유형의 Service
• ClusterIP 
• NodePort 
• LoadBalance

ClusterIP
내부 서비스에 사용된다.
 - 가상 ip 주소 로드 밸런서 요청은 백엔드 포드로 설정된다.
 - 클러스터 내 어디에서나 액세스 가능
 - 외부에서 액세스할 수 없다.

NodePort
외부 서비스에 사용된다.
 - 각 작업자 노드에 포트를 노출한다.
 - 외부에서 액세스할 수 있다.
 - 포드에 로드 밸런싱을 위해 ClusterIP를 사용한다.(RR)

LoadBalancer
외부로드 밸런서를 생성하고 관리한다.
- 수신 트래픽을 위해 NodePort를 사용할 수 있고, 포드의 로드 밸런싱을 위해 ClusterIP를 사용
- 다양하게 구현된 플러그인 사용

ingress
쿠버네티스에서 ingress는 외부 트래픽을 포드로 라우팅 하는 방법이다.
 - 7계층 트래픽에서 작동
 - HTTP 헤더를 기준으로 요청에 라우팅한다.
 - 구현에 따라 추가 기능이 있다.
 - 서로 다른 ingress 컨트롤러 사용
    – Contour – NSX-T Data Center – NSX Advanced LB (AVI) – NGINX – Traefik – Amazon Application Load Balancer (ALB)

ingress controller
ingress 컨트롤러는 osi 7계층 서비스에 사용된다.

